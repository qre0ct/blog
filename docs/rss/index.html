<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[nuckingfoob]]></title><description><![CDATA[Thoughts, stories, ideas, experiences]]></description><link>http://localhost:2368/</link><image><url>http://localhost:2368/favicon.png</url><title>nuckingfoob</title><link>http://localhost:2368/</link></image><generator>Ghost 3.4</generator><lastBuildDate>Sat, 08 Feb 2020 15:50:25 GMT</lastBuildDate><atom:link href="http://localhost:2368/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Audit AWS S3]]></title><description><![CDATA[<h3 id="what">What?</h3><p><br>Go through all the S3 buckets &amp; objects in the existing AWS infra &amp; check to see how many &amp; which of those are publicly accessible &amp; why.</p><h3 id="why">Why?</h3><ul><li>We need to get a picture of the current state of S3 in our infrastructure . This would help us assess</li></ul>]]></description><link>http://localhost:2368/audit-aws-s3/</link><guid isPermaLink="false">5e3e578704b19e5725a51317</guid><category><![CDATA[securing-aws]]></category><category><![CDATA[LeftBrain]]></category><dc:creator><![CDATA[qreoct]]></dc:creator><pubDate>Sat, 08 Feb 2020 06:46:57 GMT</pubDate><content:encoded><![CDATA[<h3 id="what">What?</h3><p><br>Go through all the S3 buckets &amp; objects in the existing AWS infra &amp; check to see how many &amp; which of those are publicly accessible &amp; why.</p><h3 id="why">Why?</h3><ul><li>We need to get a picture of the current state of S3 in our infrastructure . This would help us assess what &amp; how much work needs to be done</li><li>It would help us keep a track of our progress</li><li>This essentially defines our benchmark</li></ul><h3 id="how">How?</h3><p><br>There's a possibility that this is the first time that the S3 resource is going to be used in our AWS infra, in which case, the effects of audits may not be immediately visible. Nevertheless, audits still make sense as the usage of S3, in our infra, expands.</p><p><br>In the other case, where AWS S3 is already being used in the infra, this could easily become one of the most time taking (&amp; consuming) task. We could choose to do this manually by logging into the AWS console everyday &amp; doing this audit manually or with the power of programming/scripting (especially in python) bestowed in us, we could choose to automate the audits. (I am not a big fan of the former approach personally, <strong>at all!</strong>)<br></p><h3 id="milestones">Milestones?</h3><ul><li>get a list of all existing buckets/objects, their existing access permissions &amp; possibly their owners &amp; reasons for why these buckets/objects are public</li><li>get a count of buckets/objects that are publicly accessible</li><li>have a script ensuring that this list is regularly updated &amp; maintained<br></li></ul><p>As mentioned earlier, one way of doing the above is to goto the AWS console &amp; look for these buckets &amp; their permissions &amp; maintain a record of the same manually. However, I prefer automation wherever possible (&amp; sensible). There are plenty of open source scripts/tools that let you do these kind of audits. A simple Google search would give enough good results, like:</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/scalefactory/s3audit"><div class="kg-bookmark-content"><div class="kg-bookmark-title">scalefactory/s3audit</div><div class="kg-bookmark-description">CLI tool for auditing S3 buckets. Contribute to scalefactory/s3audit development by creating an account on GitHub.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/favicon.ico"><span class="kg-bookmark-author">scalefactory</span><span class="kg-bookmark-publisher">GitHub</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://avatars2.githubusercontent.com/u/1318638?s=400&amp;v=4"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/SecOps-Institute/AWS-S3-Buckets-Audit-Users"><div class="kg-bookmark-content"><div class="kg-bookmark-title">SecOps-Institute/AWS-S3-Buckets-Audit-Users</div><div class="kg-bookmark-description">Ever tried to summarise the User access to the S3 buckets in your AWS Account? Here is the tool that can help you do the same - SecOps-Institute/AWS-S3-Buckets-Audit-Users</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/favicon.ico"><span class="kg-bookmark-author">SecOps-Institute</span><span class="kg-bookmark-publisher">GitHub</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://avatars0.githubusercontent.com/u/37894354?s=400&amp;v=4"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/richarvey/s3-permission-checker"><div class="kg-bookmark-content"><div class="kg-bookmark-title">richarvey/s3-permission-checker</div><div class="kg-bookmark-description">Check read, write permissions on S3 buckets in your account - richarvey/s3-permission-checker</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/favicon.ico"><span class="kg-bookmark-author">richarvey</span><span class="kg-bookmark-publisher">GitHub</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://avatars0.githubusercontent.com/u/532137?s=400&amp;v=4"></div></a></figure><p>etc. All of the above are good tools that can be used to get S3 audits in place. </p><p>The above script helps us achieve all the milestones identified above, except the part that mentions "possibly their owners &amp; reasons for why these buckets/objects are public". This is a manual thing that needs to be done, unless there's already enough tooling in the existing infra that maintains this record already.</p><p>However, once we have captured the above data &amp; analyzed it, we are in a position to determine what exactly are the requirements of our developers, why do they need buckets/objects with a certain access &amp; which all of the buckets/objects can/should remain with lenient access controls. Consequently, this allows for more informed decisions on what may be called insecure in the context of our developers/our org requirements, instead of a <em>one size fits all</em> approach. It helps us decide the strategy that would best suit the custom needs of our devs while ensuring security around anything (s3 in this case).</p><p>For example, in our use case, after doing the above exercise &amp; extended discussions with our devops/systems team/enough devs, we concluded on the below strategy for managing access around our S3:</p><ol><li>Only &amp; only the following 3 operations should be allowed onto any bucket: <strong>s3:GetObject, s3:PutObject and s3:DeleteObject</strong></li><li>There should be one IAM user for every single bucket who would be allowed the above 3 access permissions <strong>onto that bucket &amp; that bucket alone</strong>. A naming convention was also made ensuring that all such IAM user names end with <code>-s3</code> so we could easily identify these users as &amp; when needed</li><li>All of these users must belong to the only one AWS account that we use, or in other words, no cross account access allowed</li></ol><p>Any buckets that do not follow the above criteria would be considered insecure. </p><blockquote><em>Now the above example is a very opinionated conclusion based on our specific requirements. This could be anything else in your case. </em></blockquote><p>So, to suit our specific audit needs, we came up with a custom audit script, which can be found here:</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/c0n71nu3/s3Auditor"><div class="kg-bookmark-content"><div class="kg-bookmark-title">c0n71nu3/s3Auditor</div><div class="kg-bookmark-description">Contribute to c0n71nu3/s3Auditor development by creating an account on GitHub.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/favicon.ico"><span class="kg-bookmark-author">c0n71nu3</span><span class="kg-bookmark-publisher">GitHub</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://avatars0.githubusercontent.com/u/11993558?s=400&amp;v=4"></div></a></figure><p>After the results of the above audit are available, the next step is to start working on the data by getting the bucket/object access fixed where ever identified as necessary. This may again be quite a manual task (&amp; a mammoth in our case), depending on how the processes are defined in your org, as it may need context, permissions, execution capabilities/bandwidth etc. to get these fixed. Once all the identified issues are fixed, we would have reached a clean slate. The audits would need to be still run periodically though to ensure that the security team is on top of things should anything come up again after the audits or to keep a track of the progress around the clean up itself. </p><blockquote>The <strong>number</strong> of buckets still existing with <em>unacceptable</em> access gives a great deal of clarity on whether efforts are being invested in the right direction or not. <em>Mangers/leadership please smile :)</em></blockquote>]]></content:encoded></item><item><title><![CDATA[Objective 1: Secure AWS S3]]></title><description><![CDATA[<p><a href="https://aws.amazon.com/s3/">What is AWS S3?</a> </p><p>Very simply put, it is a service offered by AWS that can be used as storage (called <em>buckets</em>) for different types of files. </p><p>So what does it mean to secure AWS S3?</p><p>It could mean n number of things. One of the things is to ensure</p>]]></description><link>http://localhost:2368/objective-1-secure-aws-s3/</link><guid isPermaLink="false">5e369ea804b19e5725a512b1</guid><category><![CDATA[securing-aws]]></category><category><![CDATA[LeftBrain]]></category><dc:creator><![CDATA[qreoct]]></dc:creator><pubDate>Sun, 02 Feb 2020 10:05:33 GMT</pubDate><content:encoded><![CDATA[<p><a href="https://aws.amazon.com/s3/">What is AWS S3?</a> </p><p>Very simply put, it is a service offered by AWS that can be used as storage (called <em>buckets</em>) for different types of files. </p><p>So what does it mean to secure AWS S3?</p><p>It could mean n number of things. One of the things is to ensure that the buckets &amp; it's contents (objects) are access controlled &amp; we'll focus on this aspect. </p><p><em>(Others could include things like ensuring that the bucket &amp; it's contents are protected against data loss, s3 objects are encrypted at rest, there's logging enabled for the buckets etc. We would not talk about these or any others in this case. Also, AWS by default has options to ensure public access around S3 is taken care of , like disabling public access at the account level itself. We would not talk about this either, as may not always be feasible for every org/use case, like it wasn't in our case) </em></p><h3 id="key-result-no-open-public-buckets-objects"><u>Key result</u>: No open/public buckets/objects</h3><p>We need a measurable key result to ensure that we have been able to achieve our objective. We define our key result as a measure of the number of AWS S3 buckets or any content/s within them that are publicly accessible. So ideally, if we could define <em>zero number of open/public buckets/objects </em>as our criteria to say that we have achieved our objective, nothing like it. </p><p>But of course there could be reasons for certain buckets or objects (contents of a bucket) to be publicly accessible, depending on the business context, which always is/should be the highest priority. Hence our actual key result, to accommodate for the above, becomes:</p><ul><li><em>No open/public buckets/objects, </em></li><li><em>at least not without prior approval from the security team or the information of the security team. </em></li></ul><h3 id="plan"><u>Plan</u></h3><p>Below would be our plan to reach our key result/s &amp; finally achieve our objective too. </p><ol><li>Audit &amp; ensure that the existing open buckets/objects fixed/accounted for</li><li>Ensure that any new buckets/objects being created are secure</li><li>Ensure that the security team is made aware of any insecure buckets/objects existence/creation (if at all) as quickly as possible</li></ol>]]></content:encoded></item><item><title><![CDATA[Securing your cloud infra - one step at a time]]></title><description><![CDATA[<p>So this blogpost has been sitting in my drafts for indeed a very long time. And I am definitely late to the party, but hopefully the write up is still of some help to someone.</p><p>Securing any cloud environment, for that matter, is a vast topic &amp; it would be</p>]]></description><link>http://localhost:2368/securing-cloudinfra-intro/</link><guid isPermaLink="false">5e344fc8cd85244165d16fac</guid><category><![CDATA[securing-aws]]></category><category><![CDATA[LeftBrain]]></category><dc:creator><![CDATA[qreoct]]></dc:creator><pubDate>Fri, 31 Jan 2020 16:03:29 GMT</pubDate><content:encoded><![CDATA[<p>So this blogpost has been sitting in my drafts for indeed a very long time. And I am definitely late to the party, but hopefully the write up is still of some help to someone.</p><p>Securing any cloud environment, for that matter, is a vast topic &amp; it would be difficult to cover it all in one single blogpost. Hence, I would try to break it down as per a generic approach that I usually take when trying to think of solutions around any given problem. One thing that I have learnt, sort of the hard way &amp; I am very grateful for this learning is, </p><ol><li>to identify the actual root cause of the problem</li><li>to <strong>measure what matters Â </strong>(<a href="https://books.google.co.in/books/about/Measure_What_Matters.html?id=u2NDDwAAQBAJ&amp;redir_esc=y">excellent read</a> IMHO)</li><li>collaborate (wherever &amp; whenever possible) with devs &amp; systems teams. It makes a security engineer's job a breeze &amp; solutions worthwhile !</li></ol><p>For this post we would <strong>not</strong> focus on identification of the root cause of the problem, since this post is directed towards securing your cloud infra &amp; of course because I would like to keep this post more technical than philosophical. We would <em>assume</em> that we have a problem statement at hand that needs to be solved. </p><p>For this post and (hopefully) a few follow up ones, we would focus on securing AWS, <em>one step at a time.</em> AWS itself has plenty of resources &amp; securing AWS essentially means securing each of these resources, of course depending on what resources you are using out of these. It does not make a lot of sense to try securing s3, for example, if you're not really using it at all. </p><p><u>Problem statement</u>: <em>Secure AWS infrastructure</em> </p><p>If we want to solve the above problem, we would break up the problem into smaller sub problems/objectives. </p><p><u>Objectives</u>: </p><ol><li><em>Secure AWS S3 </em></li><li><em>Secure Ec2 instances</em></li><li><em>Secure IAM</em></li><li><em>Secure EKS</em></li></ol><p>The above is a very limited list. But for now, let us focus on them alone and one at a time. </p><p><strong><u>Credits: </u></strong></p><ul><li><strong>@makash for the constant motivation</strong></li><li><strong>@amolnaik4 for guidance around thought process</strong></li><li><strong>@AjeyGore for introduction to measure what matters</strong></li></ul>]]></content:encoded></item></channel></rss>