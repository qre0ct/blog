<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[nuckingfoob]]></title><description><![CDATA[Thoughts, stories, ideas, experiences]]></description><link>http://localhost:2368/</link><image><url>http://localhost:2368/favicon.png</url><title>nuckingfoob</title><link>http://localhost:2368/</link></image><generator>Ghost 3.4</generator><lastBuildDate>Sat, 29 Feb 2020 05:02:01 GMT</lastBuildDate><atom:link href="http://localhost:2368/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[That Dream Job]]></title><description><![CDATA[Interview experience with LinkedIn around 2016]]></description><link>http://localhost:2368/that-dream-job/</link><guid isPermaLink="false">5e42cb83b9fad24fb261034c</guid><category><![CDATA[Gratitude]]></category><category><![CDATA[RightBrain]]></category><dc:creator><![CDATA[qreoct]]></dc:creator><pubDate>Tue, 11 Feb 2020 15:44:00 GMT</pubDate><content:encoded><![CDATA[<p>Not that I was desperately looking out for a change at this point in time, but appearing for the selection process of different companies, for an information security role, has always been a brutal teacher. And although I was fortunate to crack some of those, I am particularly more delightful about the other kind, for those have been the real fun ones. And then of-course, some names have always struck a keen desire in me, even before I was eligible to be employed full-time, to get an opportunity to work with their teams to experience the brilliant and that culture and values. And trust me, for names like these, I don’t really need to be ‘desperately’ looking out for a change. It’s roles/companies like these that help me understand the term ‘best’ (which is otherwise quite vague and relative) that I like to call a dream job.</p><p>LinkedIn happens to be one such phenomenon, that I would readily be very positive about, from an employment perspective, unless of-course I am already into something that’s analogous to ‘saving the world’ or am already working with people like the above. Having like minded friends is always a boon. Thanks to one such friend (<a href="https://www.linkedin.com/in/avradeep/">Avradeep Bhattacharya</a>), my profile caught the recruitment team’s attention. What follows now is a narration of my personal experience with the entire selection process.</p><p><em>It might get a little too melodramatic. But that’s how it was. You have been warned !</em></p><p>My friend confirmed about forwarding my profile to the respective hiring team on a Wednesday. The following Thursday is when I received an introductory email from the recruitment team at LinkedIn which was soon followed by a call to discuss the opportunity and my interests further. A sweet mid 20s voice with an absolute professional tone acknowledged my ‘Hello’ and off we went to the very first step of the selection procedure. I was briefed about the role, the kind of people that I would be working with, the location etc. And I think, I blabbered a lot about my experience and why exactly was I keen on this opportunity. (It was the excitement speaking, not me ! :)) But I guess I did a fair job at it, because at the end of it, the lady at the other end seemed convinced and we ended on the note that the ball was now in the hiring manager’s court and that if the manager sees my profile a fit, I would be contacted back soon.</p><p>I ‘<em>chillaxed</em>’ while waiting for a call. And the general idea is 2–3 days, or more sometimes (or sometimes no intimation at all), before you get the next call. But LinkedIn takes ‘soon’ literally seriously. ‘Ping’ ! within the next 30 minutes of the previous interaction and acceleration in beats per second. The next thing I knew, was that the next round had been arranged as the hiring manager seemed interested in giving me a shot.</p><h3 id="round-1-">Round 1:</h3><p>‘Soon’ again, there was a challenge that was shared with me. I was supposed to solve it and send back my thoughts about the same and possibly the solution too. I was given around 48 hours to complete the challenge. Now although the instructions in the challenge mentioned that ‘ideally’ it could be done within 2–3 hours at max, I wanted to take all the time available and, being the stubborn me, give it one shot after the other aiming at making it a little better each time.</p><p>Because it was and still is confidential as I understand, I may not be able to share the exact details of the challenge. However, I would like to share whatever I can being in the scope of the non-disclosure. Getting started with the instructions file, the requirements and expectations were crystal clear. The challenge itself, was quite opposite, at least from it’s first looks. It was not before some some 30–45 minutes of dedicated poking and playing around that I got a basic grasp of what exactly was I dealing with. Dinner break. Later that same evening, between shots of dark caffeine, it was another 3–4 hours it took to finally come up with a working PoC that satisfied the bare minimum expectation. Sigh! “That was cool” is what reverberated till I guess I slept it over. The following day, since I had all the time till the final hours, there was war on Stack overflow, Facebook, numerous blogs and discussion forums etc. around things I kept exploring and asking and debating to make the solution better and achieve what I wanted to. The challenge was not the challenge itself, it was the fact that the exact thing itself, being confidential, could not be shared. The questions had to be very vague and abstract and generic. And there was lot of criticism and down votes around the questions I was asking. I could not have expected for more. I mean, I was asking X in my questions and was trying to get an answer for Y. :) But at the end, although I did not receive the answer I was looking for, what I did come across through all the research etc. was n number of ways of attempting what I was trying to do and finally realize that whatever I was trying to do, was not feasible at all given the problem statement at hand. Cool right ? Anyway, the solution was finally submitted in the 11th hour. And as all the 11th hour things ought to mess up, my submission was no exception either. First I missed the attachment in the email. Then I sent the wrong attachment in the second email. And the final submission, witht the right PoC attached, had a little ambiguous instructions about how to run the PoC. :) But, it soon hit, that 3 iterations for one mail is more than enough. And I just left it there.</p><h3 id="round-2-">Round 2:</h3><p>‘Soon’ once again, my solution sailed me through into the next round.<br>A late night phone call was arranged with a senior engineer in the team, whose LinkedIn profile was shared across beforehand. (The call was actually rescheduled for later that night, at my convenience, as the interviewer had to attend to some urgent stuff at the previously scheduled time) The profile was pretty impressive. The intimidation was intense and so was my eagerness about the call. It was the high school exam days revisited when you feel that you are prepared, yet you are all apprehensive.<br>It’s a quality I think a lot of folks have picked up at LinkedIn (or maybe it’s just a character you build with maturity), the guy’s voice was extremely polite and humble, yet absolutely professional. He gave a small introduction about himself and his role. I followed his cue and gave mine as well, this time ensuring that it’s me doing the talking and not the excitement, trying to pick up something from the demeanor of the interviewer. Mostly the discussion revolved around an in depth understanding of some very basic technologies. I was not very fast in answering them, because frankly I had to think about the questions and cross-questions that were thrown depending on my answers, but I guess I could cover most of them at an ok pace. There was one question though that I could not answer despite my making an attempt at it, because I did not know about that technical aspect at all.<br>It was a good discussion I would say, for I was forced to think through problems given and not just produce an answer by the books. I could easily relate to real world problems through the situations that were presented by the interviewer. The questions all made perfect sense such that they were encapsulating real world issues and were not just theoretical out of thin air.</p><h3 id="conclusion">Conclusion</h3><p>This time it was a long wait. I was 80% sure to get back an affirmative call and an entry into the next round of the process. Alas! LinkedIn had plans otherwise. :) Being an extended weekend, I received a mail on the first working day following the long weekend informing me that the hiring team did not see me fit at that point in time for the opening. I am not certain now where did I miss it. As much as I would have liked to be promoted to the next round, I am certain there was reason enough to believe that I had something missing for that role. Upon request, I also received a feedback from LinkedIn hiring team about the areas I could improve on. But overall, I think the entire experience was really smooth and fun and above all pretty fruitful.</p><p>And of-course, I would still be looking out for one of these dream jobs, for if nothing, it is such experiences that really count.<br>Cheers ! :)</p><p><strong><em>PS</em></strong>: Now that it's been almost 3+ years since I interviewed for this position, I would try to post a tech blog post as well around the challenge itself. </p>]]></content:encoded></item><item><title><![CDATA[Yet Another Nice Discussion]]></title><description><![CDATA[Interview experience with Postman around 2016]]></description><link>http://localhost:2368/yet-another-nice-discussion/</link><guid isPermaLink="false">5e42c92bb9fad24fb261033f</guid><category><![CDATA[Gratitude]]></category><category><![CDATA[RightBrain]]></category><dc:creator><![CDATA[qreoct]]></dc:creator><pubDate>Tue, 11 Feb 2020 15:40:24 GMT</pubDate><content:encoded><![CDATA[<p>How often are you greeted by one Cooper as you walk in for a discussion with your next potential employer. Seldom, at least in the IT world. Oh and in case Cooper didn’t really ring a bell, it’s a handsome, playful ball of Retriever furs we are talking about. We bonded in the first sight and if only Cooper could speak, no alternate job offer could have ever matched that. But, guess it was better that way, for who knows Cooper would have said, “Dude, seriously, stop playing with me and focus on your interview. It’s POSTMAN not your regular stuff … woff ! woff ! “</p><p>So here’s another of those very unique and interesting experiences I had with the folks <a href="http://twitter.com/POSTMAN" rel="noopener nofollow">@POSTMAN</a>. And if you’re a developer, either you already know of POSTMAN or you are just primitive, in which case here’s something to get you back to the future: <a href="https://www.getpostman.com/apps" rel="noopener nofollow">https://www.getpostman.com/apps</a><br>Now although I have recently been lucky to have discussions, in person and over technology, with the class of people called co-founders of yet another phenomenon called start-ups, this particularly was unique with respect to quite a few things (including Cooper of course).</p><p>To begin with, it was my first time ever that I actually witnessed firsthand of what I had only heard of until now, revolutionary stuff all taking place in `that` magical garage. Be it Jobs’, Gates’, Bezos’ or that Menlopark Google’s or Disney’s, all of these magical garages were the birthplace of some of the biggest names we know of today. POSTMAN clearly was not a garage, guess primarily due to economic growth, allowing entrepreneurs to move on and think outside the garage now, but it was not a picture that anyone would usually paint of an office, a corporate or even otherwise. An apartment with 2 floors, each having a 3 BHK flat, brewing with coffee and ideas, with the hall reserved for some COD, GOW, FIFA, WWE or Cooper time, the balcony overlooking the kitchen and folks who were trying to figure out the age of a wine bottle I suppose.</p><p>But this was just superficial. The real fun was when I had this discussion with a young gentleman who looked in his late 20s, or so I thought, until he broke it across (and which anyone would otherwise also guess after speaking to him for some time) that he had been working in IT since the late 90s or so and has been a part of some great products of its time. And maybe I have not spoken to as many co-founders, but I hardly have seen a few who are so transparent in their discussions about a lot of stuff, their company, your candidature etc. and even less have I seen people sharing across their pretty insightful professional experiences with you on a first meeting. Ok, now there were a lot of instances where I was in disagreement with his views or ideology, but worth appreciating was that although he was strong about his points, he wasn’t obnoxiously arrogant about them and was instead quite open and humble to discuss about them, which to me justified his maturity.</p><p>Of the many things we discussed, the ones that fascinated me was stuff around how and why POSTMAN grew from a few hundreds to now over a million developers. Why was POSTMAN not just a make do product, but one that had a solid ideology behind its engineering designs. How exactly were the engineers die hard geeks at what they did and why were they happy doing it. And all of these was accompanied with examples, often more than one, which you could actually see in their product or their work culture. Why was ‘flat hierarchy’ and ‘management transparency’ not just 1337 speak (or so some would believe)but rather stuff you could see in front of you and relate to. What were the challenges that POSTMAN was aiming at next and what scale meant to them. And am sure, even if some of us might have had the above talk sometime, with someone, but this one is hard to beat. It was my first time that I was taken around that house (which some would prefer calling an office, not me and also not the POSTMANs am sure would), introduced to the engineers and finally even take a peek (officially, no shoulder surfing or any of those stunts) at their systems, what they were designing, what they were currently working on, what they had in the box for me, what was the roadmap ahead etc. I mean that’s like what happens after you join a company right ? It was amazing. It was a nice feeling to realize that I was considered for the offered position <a href="http://twitter.com/Postman" rel="noopener nofollow">@Postman</a>. And that reminds me, even before this entire Cooper and the following picture was laid out, there were 3 rounds of talks (technical and otherwise) that I had with the co-founders and engineers <a href="http://twitter.com/POSTMAN" rel="noopener nofollow">@POSTMAN</a> on different occasions laid over a period of a week or so. The discussions were mostly around architecturally what were they looking out from a potential candidate and questions which were, I guess, to primarily measure the technical acumen of the candidate. I have had better technical questions asked in other occasions though, but this still was nice in the terms that it sort of portrayed where they were facing difficulties and how were they planing on addressing them.</p><p>Things were all in place except for a few things that I spoke about where we were in disagreement. And although I felt that I could fill in the gap <a href="http://twitter.com/POSTMAN" rel="noopener nofollow">@POSTMAN</a> and so did they that they could match my aspirations or vice-versa, but guess this is a skill am yet to master (and it’s a hard one to), you have to make tough decisions in life. And you don’t mostly get the best of both the worlds. Compromises and shortcomings are meant to be made and accepted. And so I slept over the discussion, with the ball in my court and the next morning spoke to the same guy (who only looked in his late 20s, but actually was a C employee <a href="http://twitter.com/POSTMAN" rel="noopener nofollow">@POSTMAN</a>) expressing how hard it was to let go off the offer I had been made from POSTMAN at that point in time, due to a few, but strong enough, points that made me take that decision. Later that same day, I went through the LinkedIn profiles of some of the team I had met in person <a href="http://twitter.com/POSTMAN" rel="noopener nofollow">@POSTMAN</a> and I was not surprised that the decision I had to make was actually hard. Those guys were among the ones you would always want to learn from. Sheer brilliance. And not that I regret giving it up, but I am glad I did polish that skill I have been trying to master.</p>]]></content:encoded></item><item><title><![CDATA[Proactively Secure AWS S3]]></title><description><![CDATA[Using Hashicorp Terraform for provisioning of secure (opinionated) AWS S3 buckets]]></description><link>http://localhost:2368/proactively-secure-aws-s3/</link><guid isPermaLink="false">5e3fbdb0b9fad24fb2610116</guid><category><![CDATA[securing-aws]]></category><category><![CDATA[LeftBrain]]></category><dc:creator><![CDATA[qreoct]]></dc:creator><pubDate>Sun, 09 Feb 2020 11:21:03 GMT</pubDate><content:encoded><![CDATA[<p>In the previous blogpost we explored the status of our AWS S3 by doing audits around it. We spoke to multiple of our key stakeholders, including the devs &amp; the systems teams to understand how could we fit S3 security in the context of our specific organization. While the audits are essential (&amp; an absolutely mandatory) exercise, towards our goal, it is not really scalable. It helps with the clean up task, but doesn't really ensure that more mess is not being dumped on. Hence, it becomes crucial to figure out ways to proactively ensure that any new S3 resource creation follows a certain baseline/benchmark. So with that prelude, let's explore this section with a similar approach like the last one. </p><h3 id="what">What</h3><p>Have a system in place to ensure that any new S3 resources getting created follow a certain security benchmark.</p><h3 id="why">Why</h3><p>To ensure that new S3 resource creations are secure by default as per our contextual definition of security. This consequently leads to getting the <strong>problem of insecure S3 sorted at the root</strong>. </p><h3 id="how">How</h3><p>From the results of the last section we can infer that there are some very specific needs of our devs around AWS S3 requirements. More often that not, a very loosely access controlled S3 resource is not really needed. In the process of the audits, we also made certain rules around buckets, their access &amp; their names. So to have proactive measures implemented, we would define our controls first as a set of rules/policies &amp; then build tooling or systems to facilitate easier adoption of &amp;/or enforcement these policies. And as the last time, we would need measurable criteria to verify if we have achieved what we wanted to or not, which gets us to our milestones listed below.</p><h3 id="milestones">Milestones</h3><ul><li>[ ] A policy document detailing the rules that would define what is considered secure in the context of our organization</li><li>[ ] A system that implements/enforces this policy document</li><li>[ ] Number of violations of the above rules reported in the audits on newly created resources after the proactive system/s are implemented</li></ul><p>We had already created a list of rules in the previous blogpost. In addition to those let us say that there are a couple of more use cases that were identified over a period of time. So our extended rule/policies set now become:</p><ol><li>Only &amp; only the following 3 operations would be allowed onto any newly created bucket: s3:GetObject, s3:PutObject and s3:DeleteObject</li><li>There would be one IAM user for every single bucket who would be allowed the above 3 access permissions onto that bucket &amp; that bucket alone. </li><li>Cross account S3 access would not be allowed</li><li>Every bucket will have a subfolder that would allow any objects inside it to be world/public readable</li><li>All S3 resources would be created only with the provisioned system to do so<br></li></ol><blockquote>Now once again, the above are a very contextual set of rules &amp; policies that depend on each organization. It still makes case for an example.</blockquote><p>The second bit is to think about a system that would technically implement the above policies. One of the ways to do so would be to use <a href="https://www.terraform.io/intro/index.html">Terraform</a>. The details of what it is &amp; how it can be setup &amp; used is quite decently documented in the above link. For our use case, we would make use of the below Terraform script to ensure that any new bucket creation abides by all the rules/policies we identified above. </p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/c0n71nu3/s3ProactiveSecure"><div class="kg-bookmark-content"><div class="kg-bookmark-title">c0n71nu3/s3ProactiveSecure</div><div class="kg-bookmark-description">Terraform for securing AWS S3 proactively (opinionated) - c0n71nu3/s3ProactiveSecure</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/favicon.ico"><span class="kg-bookmark-author">c0n71nu3</span><span class="kg-bookmark-publisher">GitHub</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://avatars0.githubusercontent.com/u/11993558?s=400&amp;v=4"></div></a></figure><p>Having the terraform script solves our requirement to a big extent. </p><p>The next question, however, that arises is how/who would run this script? This aspect has to be controlled. Giving it away to anyone &amp; everyone would again wind us up in a bad state. One of the ways to do this could be have another layer in between the Terraform script (version controlled), which actually makes the infra changes, and the users who need these S3 buckets. </p><p><u>Pros of this approach</u></p><ol><li>The Terraform script itself would be version controlled &amp; source maintained (with all the respective checks &amp; controls around that system). This means that the credentials to access the underlying infra does not need to be given out to any users at all. It also ensures that only the approved rules, mentioned in the script, are used for actual resource creation. Plus the inherent benefits of audit capabilities packed with it being version controlled.  </li><li>This additional layer would be the one that the user would finally interface with. Hence, all the details of the underlying Terraform can be abstracted out, thus making it very simple for any developers to create an S3 bucket. </li></ol><p><u>Cons of this approach</u> </p><p>It becomes extremely crucial that this additional layer be very tightly controlled, especially in a situation where this system may become a solution for provisioning other infra related resources as well. It needs to have it's own tamper proof, securely maintained, audit trails around which resource creation was triggered by which user. </p><blockquote>There could possibly be many other approaches to achieve the proactive controls depending on your specific context again. </blockquote><p>One such system that readily provides exactly this capability is this awesome tool from GoJek:</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/gojek/proctor"><div class="kg-bookmark-content"><div class="kg-bookmark-title">gojek/proctor</div><div class="kg-bookmark-description">A Developer-Friendly Automation Orchestrator. Contribute to gojek/proctor development by creating an account on GitHub.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/favicon.ico"><span class="kg-bookmark-author">gojek</span><span class="kg-bookmark-publisher">GitHub</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://avatars3.githubusercontent.com/u/29785210?s=400&amp;v=4"></div></a></figure><p>When the terraform script mentioned above is used with the above tool, what it provides (from our use case's perspective) is a command to the user of the form:</p><blockquote>proctor execute create-s3-bucket --name=myBucket --public=myCustomPublicFolder</blockquote><p>and produces as output the same thing as mentioned in <a href="https://github.com/c0n71nu3/s3ProactiveSecure#output">my Github link above</a>. </p><p>Once the above systems are provisioned &amp; made available to the users, the last bit that remains is to ensure that devs (or most users in general) create any S3 resources only through the above system. This would be more of a process driven thing again, which may include removal of, say, AWS console access/capabilities of any/all users. Once again this is quite contextual depending on how things are being managed at a given organization. </p><p>With all of this we are ascertained that any new bucket creations would be as per our defined policies (&amp; technically enforced for the most part). There may be exceptions at times, which would need to be accommodated on a on-demand basis (&amp; perhaps eventually generalized &amp; made a part of the above/another system if needed). Our audits, from the last blog post, are already set up to run as a cron. And using that we can track if the proactive approach, we discussed above, has actually lead to any improvements in the creation of S3 resources. </p><p><u>Revisiting our milestones:</u></p><p>[✔︎] A policy document detailing the rules that would define what is considered secure in the context of our organization<br>[✔︎] A system that implements/enforces this policy document<br>[✔︎] Number of violations of the above rules reported in the audits on newly created resources after the proactive system/s are implemented</p><p><u>Revisiting our Objective 1: Secure AWS S3 plan</u>:</p><p>[✔︎] Audit &amp; ensure that the existing open buckets/objects fixed/accounted for<br>[✔︎] Ensure that any new buckets/objects being created are secure<br>[ ] Ensure that the security team is made aware of any insecure buckets/objects existence/creation (if at all) as quickly as possible</p><hr><p><strong><strong><em><em><u>Credits</u>:</em></em></strong></strong></p><ul><li>@<em>vjdhama for guidance around Terraform</em></li></ul>]]></content:encoded></item><item><title><![CDATA[Audit AWS S3]]></title><description><![CDATA[Automating AWS S3 audits - an opinionated approach]]></description><link>http://localhost:2368/audit-aws-s3/</link><guid isPermaLink="false">5e3e578704b19e5725a51317</guid><category><![CDATA[securing-aws]]></category><category><![CDATA[LeftBrain]]></category><dc:creator><![CDATA[qreoct]]></dc:creator><pubDate>Sat, 08 Feb 2020 06:46:57 GMT</pubDate><content:encoded><![CDATA[<h3 id="what">What </h3><p>Go through all the existing S3 buckets &amp; objects in the AWS infra &amp; check to see how many &amp; which of those are publicly accessible &amp; why.</p><h3 id="why">Why</h3><ul><li>We need to get a picture of the current state of S3 in our infrastructure . This would help us assess what &amp; how much work needs to be done</li><li>It would help us keep a track of our progress</li><li>This essentially defines our benchmark</li></ul><h3 id="how">How </h3><p>There's a possibility that this is the first time that the S3 resource is going to be used in our AWS infra, in which case, the effects of audits may not be immediately visible. Nevertheless, audits still make sense as the usage of S3, in our infra, expands. </p><p>In the other case, where AWS S3 is already being used in the infra, this could easily become one of the most time taking (&amp; consuming) task. We could choose to do this manually by logging into the AWS console everyday &amp; doing this audit manually or with the power of programming/scripting (especially in python) bestowed in us, we could choose to automate the audits. (I am not a big fan of the former approach personally, <strong>at all!</strong>)</p><h3 id="milestones">Milestones</h3><ul><li>[ ] get a list of all existing buckets/objects, their existing access permissions &amp; possibly their owners &amp; reasons for why these buckets/objects are public</li><li>[ ] get a count of buckets/objects that are publicly accessible</li><li>[ ] have a script ensuring that this list is regularly updated &amp; maintained</li></ul><p>As mentioned earlier, one way of doing the above is to goto the AWS console &amp; look for these buckets &amp; their permissions &amp; maintain a record of the same manually. However, I prefer automation wherever possible (&amp; sensible). There are plenty of open source scripts/tools that let you do these kind of audits. A simple Google search would give enough good results, like:</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/scalefactory/s3audit"><div class="kg-bookmark-content"><div class="kg-bookmark-title">scalefactory/s3audit</div><div class="kg-bookmark-description">CLI tool for auditing S3 buckets. Contribute to scalefactory/s3audit development by creating an account on GitHub.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/favicon.ico"><span class="kg-bookmark-author">scalefactory</span><span class="kg-bookmark-publisher">GitHub</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://avatars2.githubusercontent.com/u/1318638?s=400&amp;v=4"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/SecOps-Institute/AWS-S3-Buckets-Audit-Users"><div class="kg-bookmark-content"><div class="kg-bookmark-title">SecOps-Institute/AWS-S3-Buckets-Audit-Users</div><div class="kg-bookmark-description">Ever tried to summarise the User access to the S3 buckets in your AWS Account? Here is the tool that can help you do the same - SecOps-Institute/AWS-S3-Buckets-Audit-Users</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/favicon.ico"><span class="kg-bookmark-author">SecOps-Institute</span><span class="kg-bookmark-publisher">GitHub</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://avatars0.githubusercontent.com/u/37894354?s=400&amp;v=4"></div></a></figure><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/richarvey/s3-permission-checker"><div class="kg-bookmark-content"><div class="kg-bookmark-title">richarvey/s3-permission-checker</div><div class="kg-bookmark-description">Check read, write permissions on S3 buckets in your account - richarvey/s3-permission-checker</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/favicon.ico"><span class="kg-bookmark-author">richarvey</span><span class="kg-bookmark-publisher">GitHub</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://avatars0.githubusercontent.com/u/532137?s=400&amp;v=4"></div></a></figure><p>etc. All of the above are good tools that can be used to get S3 audits in place. </p><p>The above script helps us achieve all the milestones identified above, except the part that mentions "possibly their owners &amp; reasons for why these buckets/objects are public". This is a manual thing that needs to be done, unless there's already enough tooling in the existing infra that maintains this record already.</p><p>However, once we have captured the above data &amp; analyzed it, we are in a position to determine what exactly are the requirements of our developers, why do they need buckets/objects with a certain access &amp; which all of the buckets/objects can/should remain with lenient access controls. Consequently, this allows for more informed decisions on what may be called insecure in the context of our developers/our org requirements, instead of a <em>one size fits all</em> approach. It helps us decide the strategy that would best suit the custom needs of our devs while ensuring security around anything (s3 in this case).</p><p>For example, in our use case, after doing the above exercise &amp; extended discussions with our devops/systems team/enough devs, we concluded on the below strategy for managing access around our S3:</p><ol><li>Only &amp; only the following 3 operations should be allowed onto any bucket: <strong>s3:GetObject, s3:PutObject and s3:DeleteObject</strong></li><li>There should be one IAM user for every single bucket who would be allowed the above 3 access permissions <strong>onto that bucket &amp; that bucket alone</strong>. A naming convention was also made ensuring that all such IAM user names end with <code>-s3</code> so we could easily identify these users as &amp; when needed</li><li>All of these users must belong to the only one AWS account that we use, or in other words, no cross account access allowed</li></ol><p>Any buckets that do not follow the above criteria would be considered insecure. </p><blockquote><em>Now the above example is a very opinionated conclusion based on our specific requirements. This could be anything else in your case. </em></blockquote><p>So, to suit our specific audit needs, we came up with a custom audit script, which can be found here:</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/c0n71nu3/s3Auditor"><div class="kg-bookmark-content"><div class="kg-bookmark-title">c0n71nu3/s3Auditor</div><div class="kg-bookmark-description">Contribute to c0n71nu3/s3Auditor development by creating an account on GitHub.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.githubassets.com/favicon.ico"><span class="kg-bookmark-author">c0n71nu3</span><span class="kg-bookmark-publisher">GitHub</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://avatars0.githubusercontent.com/u/11993558?s=400&amp;v=4"></div></a></figure><p>After the results of the above audit are available, the next step is to start working on the data by getting the bucket/object access fixed where ever identified as necessary. This may again be quite a manual task (&amp; a mammoth in our case), depending on how the processes are defined in your org, as it may need context, permissions, execution capabilities/bandwidth etc. to get these fixed. Once all the identified issues are fixed, we would have reached a clean slate. The audits would need to be still run periodically though to ensure that the security team is on top of things should anything come up again after the audits or to keep a track of the progress around the clean up itself. </p><blockquote>The <strong>number</strong> of buckets still existing with <em>unacceptable</em> access gives a great deal of clarity on whether efforts are being invested in the right direction or not. <em>Mangers/leadership please smile :)</em></blockquote><p><u>Revisiting our milestones</u>:</p><ul><li>[✔︎] get a list of all existing buckets/objects, their existing access permissions &amp; possibly their owners &amp; reasons for why these buckets/objects are public</li><li>[✔︎] get a count of buckets/objects that are publicly accessible</li><li>[✔︎] have a script ensuring that this list is regularly updated &amp; maintained</li></ul><p><u>Revisiting our Objective 1: Secure AWS S3 plan</u>:</p><p>[✔︎] Audit &amp; ensure that the existing open buckets/objects fixed/accounted for<br>[ ] Ensure that any new buckets/objects being created are secure<br>[ ] Ensure that the security team is made aware of any insecure buckets/objects existence/creation (if at all) as quickly as possible</p>]]></content:encoded></item><item><title><![CDATA[Objective 1: Secure AWS S3]]></title><description><![CDATA[Securing AWS S3 - Strategy & planning that worked for us]]></description><link>http://localhost:2368/objective-1-secure-aws-s3/</link><guid isPermaLink="false">5e369ea804b19e5725a512b1</guid><category><![CDATA[securing-aws]]></category><category><![CDATA[LeftBrain]]></category><dc:creator><![CDATA[qreoct]]></dc:creator><pubDate>Sun, 02 Feb 2020 10:05:33 GMT</pubDate><content:encoded><![CDATA[<p><a href="https://aws.amazon.com/s3/">What is AWS S3?</a> </p><p>Very simply put, it is a service offered by AWS that can be used as storage (called <em>buckets</em>) for different types of files. </p><p>So what does it mean to secure AWS S3?</p><p>It could mean n number of things. One of the things is to ensure that the buckets &amp; it's contents (objects) are access controlled &amp; we'll focus on this aspect. </p><p><em>(Others could include things like ensuring that the bucket &amp; it's contents are protected against data loss, s3 objects are encrypted at rest, there's logging enabled for the buckets etc. We would not talk about these or any others in this case. Also, AWS by default has options to ensure public access around S3 is taken care of , like disabling public access at the account level itself. We would not talk about this either, as may not always be feasible for every org/use case, like it wasn't in our case) </em></p><h3 id="key-result-no-open-public-buckets-objects"><u>Key result</u>: No open/public buckets/objects</h3><p>We need a measurable key result to ensure that we have been able to achieve our objective. We define our key result as a measure of the number of AWS S3 buckets or any content/s within them that are publicly accessible. So ideally, if we could define <em>zero number of open/public buckets/objects </em>as our criteria to say that we have achieved our objective, nothing like it. </p><p>But of course there could be reasons for certain buckets or objects (contents of a bucket) to be publicly accessible, depending on the business context, which always is/should be the highest priority. Hence our actual key result, to accommodate for the above, becomes:</p><ul><li><em>No open/public buckets/objects, </em></li><li><em>at least not without prior approval from the security team or the information of the security team. </em></li></ul><h3 id="plan"><u>Plan</u></h3><p>Below would be our plan to reach our key result/s &amp; finally achieve our objective too. </p><ol><li>Audit &amp; ensure that the existing open buckets/objects fixed/accounted for</li><li>Ensure that any new buckets/objects being created are secure</li><li>Ensure that the security team is made aware of any insecure buckets/objects existence/creation (if at all) as quickly as possible</li></ol>]]></content:encoded></item><item><title><![CDATA[Securing Your Cloud Infra]]></title><description><![CDATA[A first in a series of blog posts on approaching cloud security around AWS & GCP, with anecdotal examples]]></description><link>http://localhost:2368/securing-cloudinfra-intro/</link><guid isPermaLink="false">5e344fc8cd85244165d16fac</guid><category><![CDATA[securing-aws]]></category><category><![CDATA[LeftBrain]]></category><dc:creator><![CDATA[qreoct]]></dc:creator><pubDate>Fri, 31 Jan 2020 16:03:29 GMT</pubDate><content:encoded><![CDATA[<p>So this blogpost has been sitting in my drafts for indeed a very long time. And I am definitely late to the party, but hopefully the write up is still of some help to someone.</p><p>Securing any cloud environment, for that matter, is a vast topic &amp; it would be difficult to cover it all in one single blogpost. Hence, I would try to break it down as per a generic approach that I usually take when trying to think of solutions around any given problem. Also, we would try to have the blogpost designed with 2 things in mind:</p><ul><li>we would approach it one step at a time</li><li>we would try to keep our solutions as unblocking as possible for our devs</li></ul><p>A few things that I have learnt, sort of the hard way &amp; I am very grateful for this learning, is: </p><ol><li>to identify the actual root cause of the problem</li><li>to <strong>measure what matters  </strong>(<a href="https://books.google.co.in/books/about/Measure_What_Matters.html?id=u2NDDwAAQBAJ&amp;redir_esc=y">excellent read</a> IMHO)</li><li>collaborate (wherever &amp; whenever possible) with devs &amp; systems teams. It makes a security engineer's job a breeze &amp; solutions worthwhile !</li></ol><p>For this post we would <strong>not</strong> focus on identification of the root cause of the problem, since this post is directed towards securing your cloud infra &amp; of course because I would like to keep this post more technical than philosophical. We would <em>assume</em> that we have a problem statement at hand that needs to be solved. </p><p>For this post and (hopefully) a few follow up ones, we would focus on securing AWS, <em>one step at a time.</em> AWS itself has plenty of resources &amp; securing AWS essentially means securing each of these resources, of course depending on what resources you are using out of these. It does not make a lot of sense to try securing s3, for example, if you're not really using it at all. </p><p><u>Problem statement</u>: <em>Secure AWS infrastructure</em> </p><p>If we want to solve the above problem, we would break up the problem into smaller sub problems/objectives. </p><p><u>Objectives</u>: </p><ol><li><em>Secure AWS S3 </em></li><li><em>Secure Ec2 instances</em></li><li><em>Secure IAM</em></li><li><em>Secure EKS</em></li></ol><p>The above is a very limited list. But for now, let us focus on them alone and one at a time. </p><hr><p><strong><em><u>Credits</u>:</em></strong></p><ul><li><em>@makash for the constant motivation</em></li><li><em>@amolnaik4 for guidance around thought process</em></li><li><em>@AjeyGore for introduction to measure what matters</em></li></ul>]]></content:encoded></item></channel></rss>